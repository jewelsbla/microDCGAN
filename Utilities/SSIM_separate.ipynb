{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to run the SSIM calculation in a subsequent step. Use a small number of images (e.g. 4) as input samples. This Program returns their nearest neighbors based on SSIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\sk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm      #progress bar\n",
    "\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = '../data/input/256x256/offset_x4'\n",
    "path_output='.'\n",
    "name_output='DCGAN_256x256'\n",
    "format_input = '.jpg'\n",
    "path_input_samples = './generated'\n",
    "format_input_samples = '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(name, array):\n",
    "    with open(name, 'w') as file:\n",
    "        # using csv.writer method from CSV package\n",
    "        write = csv.writer(file)\n",
    "        \n",
    "        write.writerow(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the following transformations:\n",
      "\n",
      "<module 'torchvision.transforms' from 'C:\\\\Users\\\\sk\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\torchvision\\\\transforms\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of transfomations\n",
    "transforms1 = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "#transforms = transforms.Compose([transforms.Resize(resize_input), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # resize operation is outsourced\n",
    "print('Applying the following transformations:\\n\\n' + str(transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5873188f1e42fab42d6e10df82f8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = []\n",
    "\n",
    "for file in (pbar := tqdm(glob.glob(path_input + '/*' + format_input))):                   # Get training data from specified folder\n",
    "    pbar.set_description('Loading images from directory')\n",
    "    img = Image.open(file)                      # Use PIL to load images\n",
    "    img_tensor = transforms1(img)                # Apply the transformations defined above, 'torch.Size([1, size_input, size_input])' with only one color channel (gray image)\n",
    "    train_set.append(img_tensor)       # Append tupel ( , )\n",
    "    \n",
    "# Shorter but less customizable method:\n",
    "#dataset = datasets.ImageFolder('../2DGAN_Data/input_256x256_debug', transform=transforms)  \n",
    "# The images are labeled with the class taken from the directory name (more suited for input data with different labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the following transformations:\n",
      "\n",
      "<module 'torchvision.transforms' from 'C:\\\\Users\\\\sk\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\torchvision\\\\transforms\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of transfomations\n",
    "transforms2 = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.Grayscale()]\n",
    ")\n",
    "\n",
    "#transforms = transforms.Compose([transforms.Resize(resize_input), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # resize operation is outsourced\n",
    "print('Applying the following transformations:\\n\\n' + str(transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63cda919114ea295082332a87103d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m     generated_samples\u001b[38;5;241m.\u001b[39mappend(img_tensor)       \u001b[38;5;66;03m# Append tupel ( , )\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Shorter but less customizable method:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#dataset = datasets.ImageFolder('../2DGAN_Data/input_256x256_debug', transform=transforms)  \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# The images are labeled with the class taken from the directory name (more suited for input data with different labels) \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerated_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "generated_samples = []\n",
    "#random.shuffle(train_list) # not necessary, DataLoader shuffles both the batches and the input images\n",
    "\n",
    "for file in (pbar := tqdm(glob.glob(path_input_samples + '/*' + '.png'))):                   # Get training data from specified folder\n",
    "    pbar.set_description('Loading images from directory')\n",
    "    img = Image.open(file)                      # Use PIL to load images\n",
    "    img_tensor = transforms2(img)                # Apply the transformations defined above, 'torch.Size([1, size_input, size_input])' with only one color channel (gray image)\n",
    "    generated_samples.append(img_tensor)       # Append tupel ( , )\n",
    "    \n",
    "# Shorter but less customizable method:\n",
    "#dataset = datasets.ImageFolder('../2DGAN_Data/input_256x256_debug', transform=transforms)  \n",
    "# The images are labeled with the class taken from the directory name (more suited for input data with different labels) \n",
    "print(generated_samples[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "size_input=256\n",
    "\n",
    "# returns enumerate with index of k nearest neighbours in training set and the corresponding distance value\n",
    "def get_k_nearest_samples(fake_image, training_images, k=4):\n",
    "    '''\n",
    "    Searches for the k-nearest samples in the dataset of a given image based on the euclidean distance.\n",
    "    '''\n",
    "    def distance(tensor_a, tensor_b):\n",
    "        '''\n",
    "        Calculates the euklidean distance of two torch tensors of the same size.\n",
    "        '''\n",
    "        return ssim(tensor_a.view(1,1,size_input,size_input), tensor_b.view(1,1,size_input,size_input))\n",
    "    \n",
    "    dist = [distance(fake_image, real_image) for real_image in tqdm(training_images)]\n",
    "    \n",
    "\n",
    "    return np.argsort(dist)[::-1][:k], np.sort(dist)[::-1][:k] # argsort returns the indices that sort the array; index, distance value\n",
    "    \n",
    "    \n",
    "# Get necessary images\n",
    "train_imgs = train_set    # get array of real images without corresponding label\n",
    "generated_samples = generated_samples\n",
    "\n",
    "\n",
    "# index = get_k_nearest_samples(generated_samples[0], train_imgs, k=1)\n",
    "# print(index)\n",
    "#print(x)\n",
    "#print(list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "eukl_dist = [(get_k_nearest_samples(generated_samples[i], train_imgs, k=1)[1])[0] for i in tqdm(range(batch_size))]\n",
    "print(eukl_dist)\n",
    "\n",
    "### Plot euklidean distance for all images samples in generated batch\n",
    "fig, ax1 = plt.subplots(figsize=(30, 10))\n",
    "ax1.plot(np.arange(1, batch_size+1, 1), np.sort(eukl_dist), color=\"darkorange\")        # use if statement to not show duplicates at each iteration\n",
    "ax1.set_title('Minimum euklidean distance of all images in one batch (sorted)')\n",
    "ax1.set_xlabel(\"Image number\")\n",
    "ax1.set_ylabel(\"Euklidean Distance\")\n",
    "\n",
    "plt.axhline(y=np.mean(eukl_dist), color='black', linestyle='--', label= 'Mean euklidean distance: %.4f (Standard deviation: %.4f)' % (np.mean(eukl_dist), np.std(eukl_dist)))\n",
    "ax1.legend(loc=\"upper right\")\n",
    "#ax1.set_xticks(np.arange(1, batch_size+1, 1.0))\n",
    "#ax1.set_xlim(1, 24)\n",
    "ax1.grid()\n",
    "to_csv(path_output+'/'+name_output+'_NN_SSIM.csv', np.sort(eukl_dist))\n",
    "plt.savefig(path_output+'/'+name_output+'_nearest_Neighbour_SSIM.png', dpi=250)\n",
    "\n",
    "# print('==========================================================================================')\n",
    "# print('Mean euklidean distance:', np.mean(eukl_dist))\n",
    "# print('Standard deviation:', np.std(eukl_dist))\n",
    "# print('==========================================================================================')\n",
    "\n",
    "#ax1.legend(loc=\"upper right\")\n",
    "'''\n",
    "\n",
    "k = 4               # number of nearest neighbours to be displayed\n",
    "eukl_index = [(get_k_nearest_samples(generated_samples[i], train_imgs, k=k)[0]) for i in range(2)]\n",
    "print(eukl_index)\n",
    "\n",
    "\n",
    "### Plot nearest neighbours\n",
    "for i in tqdm(range(4)):\n",
    "    f, ax = plt.subplots(1,k+1, figsize=(30, 10))\n",
    "    ax[0].imshow(generated_samples[i].reshape(size_input, size_input), cmap=\"gray\")\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_title(\"Generated\")\n",
    "    for axis in ['top', 'bottom', 'left', 'right']:\n",
    "        ax[0].spines[axis].set_linewidth(2.5)  # change width\n",
    "        ax[0].spines[axis].set_color('darkorange')    # change color\n",
    "    \n",
    "    for j in range(k):\n",
    "        ax[j+1].imshow(train_imgs[(eukl_index[i])[j]].reshape(size_input, size_input), cmap=\"gray\")  # display only 0,4,8 because otherwise the first images are the same, just augmented (flipped)\n",
    "        ax[j+1].axis('off')\n",
    "        ax[j+1].set_title(f\"Nearest Neighbour {j+1}\")\n",
    "        # ax[2].imshow(samples[x[1]].reshape(size_input, size_input), cmap=\"gray\")\n",
    "        # ax[2].axis('off')\n",
    "        # ax[2].set_title(\"Nearest Neighbour 2\")\n",
    "        # ax[3].imshow(samples[x[2]].reshape(size_input, size_input), cmap=\"gray\")\n",
    "        # ax[3].axis('off')\n",
    "        # ax[3].set_title(\"Nearest Neighbour 3\")\n",
    "    plt.savefig(path_output+'/'+name_output+f'_nearest_neighbour_SSIM_{i}.png', dpi=250)\n",
    "    plt.savefig(path_output+'/'+name_output+f'_nearest_neighbour_SSIM_{i}.eps', format='eps')\n",
    "\n",
    "print('SSIM finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
